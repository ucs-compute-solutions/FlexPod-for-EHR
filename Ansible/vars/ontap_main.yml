---
# Role variables as per NetApp's prescriptive guidance
# This can be overridden by a var-file.yml at the command line
# User's input variables

##################################################################################################################################################
# Cluster specific variables
##################################################################################################################################################

#Name of the ONTAP Cluster
cluster_name: A400

#Location of the ONTAP Cluster
cluster_location: SFO

#Cluster management LIF already exists (pre-requisite), so note down name of cluster management LIF and enter below.
cluster_mgmt_interface: cluster_mgmt #network interface show

#List the ONTAP Licenses for the different features that you need
ontap_license:
  #- <License key for feature 1>
  #- <License key for feature 2>
  #- <License key for feature 3>
  - KSPPZLZOFBAAAAAAAAAAAAAAAAAA   #NFS
  - SQEMCMZOFBAAAAAAAAAAAAAAAAAA  #FCP
  - AVQUMMZOFBAAAAAAAAAAAAAAAAAA  # NVME
  - IHLTAMZOFBAAAAAAAAAAAAAAAAAA #iscsi

#Details for configuring NetApp AutoSupport
autosupport_vars:
  mail_hosts: "mailhost"
  noteto: "admin@netapp.com"
  proxy_url: #Optional: If authentication is used, use format: "username:password@host:port"
  from_address: "ontap_cluster@netapp.com"
  to_addresses: admin1@netapp.com,admin2@netapp.com

#SNMP related variables
enable_snmp: true
#Please make sure to fill out the below details if you have chosen to enable SNMP, leaving it empty will cause the setup to fail
snmp_contact: "Administrator"
snmp_location: "New York"
traphost_ip: "192.168.10.10"
snmp_community: "FlexPod"

#SNMPv3 related variables
user: snmpv3_user   #The name of the user to manage
authentication_protocol: md5   #Authentication protocol for the snmp user (Choices: none, md5, sha, sha2-256)
authentication_password: Netapp!23   #Password for the authentication protocol  <enter password>
privacy_protocol: none   #Privacy protocol for the snmp user (Choices: none, des, aes128)
privacy_password:  Netapp!23  #Password for the privacy protocol <enter password>

##################################################################################################################################################
# Node/ Controller level information
##################################################################################################################################################

ha_pairs:
  - ha_no: 1
    node_port_count: "4"
    node_data_ports: ["e0e","e0f","e0g","e0h"] # network port show
    node_fcp_ports: ["1a","1b"] # fcp adapter show
    fcp_port_speed: "32" # fcp adapter show -fields speed
    node_specs:
    - node_name: A400-01 # node show
      sp: {ip: 10.29.164.83, mask: 255.255.255.0, gateway: 10.29.164.1}  # system service-processor show
      node_mgmt_ip: 10.29.132.121 # network interface show
      partner_mgmt_ip: 10.29.132.122 # network interface show
      data_aggregates: # use as needed storage aggregate show
        - {aggr_name: A400_01_NVME_SSD_2, disk_type: SSD-NVM, diskcount: 23}  #Options for disk_type: SAS, SSD, SSD-NVM 
      nfs_lifs: {name: nvme-svm-nfs-lif01, address: 10.10.42.102, netmask: 255.255.255.0}  #Fill out this value only if nfs will be mentioned under allowed_protocols in svm_specs
      fcp_lifs:    #Fill out this value only if fcp will be mentioned under allowed_protocols in svm_specs
        - {name: nvme-svm-fc-lif01a, home_port: 1a, fabric: A}  #Do not change the fabric ID
        - {name: nvme-svm-fc-lif01b, home_port: 1b, fabric: B}  #Do not change the fabric ID
      fc-nvme_lifs:    #Fill out this value only if nvme will be mentioned under allowed_protocols in svm_specs
        - {name: nvme-svm-nvme-lif01a, home_port: 1a, fabric: A}  #Do not change the fabric ID
        - {name: nvme-svm-nvme-lif01b, home_port: 1b, fabric: B}  #Do not change the fabric ID
      iscsi_lifs:  #Fill out this value only if iscsi will be mentioned under allowed_protocols in svm_specs. Provide one iSCSI LIF per iSCSI VLAN
        - {name: iscsi-lif-01a, address: 192.168.40.11, netmask: 255.255.255.0, fabric: A}  #Do not change the fabric ID
        - {name: iscsi-lif-01b, address: 192.168.50.11, netmask: 255.255.255.0, fabric: B}  #Do not change the fabric ID    
    
    - node_name: A400-02 # node show  # system service-processor show
      sp: {ip: 10.29.164.84, mask: 255.255.255.0, gateway: 10.29.164.1}
      node_mgmt_ip: 10.29.132.122  # network interface show
      partner_mgmt_ip: 10.29.132.121
      data_aggregates:  # storage aggregate show
        - {aggr_name: A400_02_NVME_SSD_2, disk_type: SSD-NVM, diskcount: 23}  #Options for disk_type: SAS, SSD, SSD-NVM     
      nfs_lifs: {name: nvme-svm-nfs-lif02, address: 10.10.42.103, netmask: 255.255.255.0}  #Fill out this value only if nfs will be mentioned under allowed_protocols in svm_specs
      fcp_lifs:    #Fill out this value only if fcp will be mentioned under allowed_protocols in svm_specs
        - {name: nvme-svm-fc-lif02a, home_port: 1a, fabric: A}  #Do not change the fabric ID
        - {name: nvme-svm-fc-lif02a, home_port: 1b, fabric: B}  #Do not change the fabric ID
      fc-nvme_lifs:    #Fill out this value only if nvme will be mentioned under allowed_protocols in svm_specs
        - {name: nvme-svm-nvme-lif02a, home_port: 1a, fabric: A}  #Do not change the fabric ID
        - {name: nvme-svm-nvme-lif02b, home_port: 1b, fabric: B}  #Do not change the fabric ID
      iscsi_lifs:  #Fill out this value only if iscsi will be mentioned under allowed_protocols in svm_specs. Provide one iSCSI LIF per iSCSI VLAN
        - {name: iscsi-lif-02a, address: 192.168.40.13, netmask: 255.255.255.0, fabric: A}  #Do not change the fabric ID
        - {name: iscsi-lif-02b, address: 192.168.50.13, netmask: 255.255.255.0, fabric: B}  #Do not change the fabric ID

##################################################################################################################################################
#SVM specific variables
##################################################################################################################################################

svm_specs:
  svm_name: hc-svm-nvme
  svm_root_vol: hc-svm-nvme_root
  allowed_protocols: #provide the values in lower case only, supported options for this solution are nfs, fcp, iscsi, nvme
    - nfs
    - fcp
    - nvme
  client_match: 10.10.42.0/24  # This should be the NFS VLAN
  data_protocol: nfs
  data_volumes:
    - {name:  infra_datastore01, size: 500, residing_aggr:  A400_02_NVME_SSD_2}
    #- {name:  infra_datastore_02, size: 1024, residing_aggr: aggr01_node02_nvme}
  swap_volumes:
    - {name: infra_swap, size: 100, residing_aggr: A400_02_NVME_SSD_2}
  nvme_datastores:
    - {name: nvme_datastore01, size: 500, residing_aggr: A400_01_NVME_SSD_1} #Use the data aggregates with disk_type as SSD-NVM
    #- {name: NVMe_datastore_04, size: 100, residing_aggr: A400_02_NVME_SSD_2}   
  data_luns:
  boot_volumes:
    - {name: esxi_boot, size: 400, residing_aggr: A400_02_NVME_SSD_1}
  boot_luns:
    - {name: esxi1, size: 64, residing_vol: esxi_boot}
    - {name: esxi2, size: 64, residing_vol: esxi_boot}
    - {name: esxi3, size: 64, residing_vol: esxi_boot}
    - {name: esxi4, size: 64, residing_vol: esxi_boot}
  nvme_specs:
    subsystem: 
      - {name: nvme_host_01_02_03_04, nqn: ["nqn.2014-08.com.vmware:nvme:esxi1-2","nqn.2014-08.com.vmware:nvme:esxi1-5","nqn.2014-08.com.vmware:nvme:esxi2-1","nqn.2014-08.com.vmware:nvme:esxi2-2"]}  #Name of the subsystem and obtain all the NQNs from the host
    namespaces:  #Mention the namespaces that you want to create and map to this subsystem
      - {name: nvme_namespace01, size: 400, residing_vol: nvme_datastore01}  #Use NVMe datastore as residing volume here
      #- {name: NVMe_namespace_02, size: 50, residing_vol: NVMe_datastore_02}  #Enter size values in gb
  
  ## No iSCSI
  iscsi_igroups:
    - {name: VM-Host-Infra-01-ISCSI, iqn: ["iqn.1992-08.com.cisco:ucs-host:1"]}
    - {name: VM-Host-Infra-02-ISCSI, iqn: ["iqn.1992-08.com.cisco:ucs-host:2"]}
  
  fcp_igroups:
    - {name: Epic-1-2, wwpn: ["20:00:00:25:b5:fc:0a:01","20:00:00:25:b5:fc:0b:01"]} # These WWPN are defined in /group_vars/all.yml. Shouldn't need to re-enter
    - {name: Epic-1-5, wwpn: ["20:00:00:25:b5:fc:0b:00","20:00:00:25:b5:fc:0a:00"]}
    - {name: Epic-2-1, wwpn: ["20:00:00:25:b5:fc:0a:02","0:00:00:25:b5:fc:0b:02"]}
    - {name: Epic-2-2, wwpn: ["20:00:00:25:b5:fc:0a:03","20:00:00:25:b5:fc:0b:03"]}
  svm_mgmt_lif: {home_node: A400-01, home_port: a0a-41, address: 10.10.41.110, netmask: 255.255.255.0, gateway: 10.10.41.1, lif_name: hc-nvme-mgmt}
  vsadmin_password: Netapp!23
  os_type: vmware
  dns_server_svm:  # DNS entries are defined in /group_vars/all.yml. Shouldn't need to re-enter
    - 10.10.40.4
    #- 192.168.20.21
  dns_domain_svm: "hc.cvd"   # dns_domain_name: is defined in /group_vars/all.yml. Shouldn't need to re-enter

##################################################################################################################################################
# Default/Best Practice related information - Change only if required
##################################################################################################################################################

#Following variable is used in a task to ensure auto revert for cluster management LIF is set to True.
cluster_mgmt_auto_revert: true

#Name of the Interface group to be created
ifgrp_name: a0a

ifgrp_mode: multimode_lacp

#Job Schedule
job_schedule:
  - {job_name: 15min,job_minutes: 15}
